{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import array\n",
    "import time\n",
    "\n",
    "SubgraphMeta = collections.namedtuple('SubgraphMeta', 'typ rel ent siz')\n",
    "Subgraph = collections.namedtuple('Subgraph', 'emb meta')\n",
    "Result = collections.namedtuple('Result', 'query posh post')\n",
    "   \n",
    "inputfile = '/var/scratch/uji300/kgemb/fb15k_avgsubgraphs.bin'\n",
    "logfile = '/var/scratch/uji300/kgemb/fb15K_subgraphs_valid.txt'\n",
    "emb_meta_e = '/var/scratch/uji300/kgemb/models/fb15k/best-model/E-meta'\n",
    "emb_e_path = '/var/scratch/uji300/kgemb/models/fb15k/best-model/E.0'\n",
    "emb_meta_r = '/var/scratch/uji300/kgemb/models/fb15k/best-model/R-meta'\n",
    "emb_r_path = '/var/scratch/uji300/kgemb/models/fb15k/best-model/R.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the file with the subgraph embeddings produced from Trident (for now I don't need them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subgraphs_meta = []\n",
    "embeddings = []\n",
    "with open(inputfile, 'rb') as fin:\n",
    "    b_nsubgraphs = fin.read(8)\n",
    "    nsubgraphs = int.from_bytes(b_nsubgraphs, byteorder='little', signed=False)\n",
    "    for i in range(nsubgraphs):\n",
    "        line = fin.read(25)\n",
    "        typ = line[0]\n",
    "        rel = int.from_bytes(line[1:9], byteorder='little', signed=False)\n",
    "        ent = int.from_bytes(line[9:17], byteorder='little', signed=False)\n",
    "        siz = int.from_bytes(line[17:], byteorder='little', signed=False)        \n",
    "        sg = SubgraphMeta(typ=typ, ent=ent, siz=siz, rel=rel)\n",
    "        subgraphs_meta.append(sg)\n",
    "    # Load the average embeddings\n",
    "    emb_meta = fin.read(10)\n",
    "    dims = int.from_bytes(emb_meta[:2], byteorder='big', signed=False)\n",
    "    mincard = int.from_bytes(emb_meta[2:], byteorder='big', signed=False)\n",
    "    for i in range(nsubgraphs):\n",
    "        b_emb = fin.read(dims * 8)\n",
    "        emb = np.frombuffer(b_emb, dtype=np.float64)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "subgraphs = []\n",
    "for i in range(nsubgraphs):\n",
    "    subgraphs.append(Subgraph(emb=embeddings[i], meta=subgraphs_meta[i]))\n",
    "subgraphs_meta = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(meta, e_path):\n",
    "    batch_size = 0\n",
    "    dim = 0\n",
    "    n = 0\n",
    "    with open(meta, 'rb') as fmeta:\n",
    "        raw = fmeta.read(10)\n",
    "        batch_size = int.from_bytes(raw[:4], byteorder='little', signed=False)\n",
    "        n = int.from_bytes(raw[4:8], byteorder='little', signed=False)\n",
    "        dim = int.from_bytes(raw[8:], byteorder='little', signed=False)\n",
    "    e = np.zeros(shape=(n,dim))\n",
    "    with open(e_path, 'rb') as fin:\n",
    "        for i in range(n):\n",
    "            line = fin.read(8 + dim * 8)            \n",
    "            emb = np.frombuffer(line[8:], dtype=np.float64)\n",
    "            e[i] = emb\n",
    "    return e\n",
    "emb_e = load_embeddings(emb_meta_e, emb_e_path)\n",
    "emb_r = load_embeddings(emb_meta_r, emb_r_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_valid_data(logfile, graph_type='POS'):\n",
    "    results = []\n",
    "    with open(logfile, 'rt') as fin:\n",
    "        header = fin.readline()\n",
    "        for l in fin:\n",
    "            tkns = l.split('\\t')\n",
    "            query = tkns[0]\n",
    "            pos_answer_subgraph_head = int(tkns[1])\n",
    "            pos_answer_subgraph_tail = int(tkns[2])\n",
    "            results.append(Result(query, pos_answer_subgraph_head, pos_answer_subgraph_tail))\n",
    "    # Create the training data\n",
    "    data_pos = np.zeros((len(results), 3), dtype=np.int)\n",
    "    data_spo = np.zeros((len(results), 3), dtype=np.int)\n",
    "    for i in range(len(results)):\n",
    "        query = results[i].query\n",
    "        tkns = query.split(' ')\n",
    "        h = int(tkns[0])\n",
    "        r = int(tkns[1])\n",
    "        t = int(tkns[2])\n",
    "        \n",
    "        \n",
    "        data_pos[i][0] = r\n",
    "        data_pos[i][1] = t\n",
    "        \n",
    "        data_spo[i][0] = r\n",
    "        data_spo[i][1] = h\n",
    "        \n",
    "        posh = results[i].posh\n",
    "        post = results[i].post\n",
    "        \n",
    "        if posh > 0:\n",
    "            if posh < 3:\n",
    "                data_pos[i][2] = 1\n",
    "            elif  posh < 5:\n",
    "                data_pos[i][2] = 2\n",
    "            elif posh < 10:\n",
    "                data_pos[i][2] = 3\n",
    "            else:\n",
    "                data_pos[i][2] = 4\n",
    "        \n",
    "        if post > 0:\n",
    "            if post < 3:\n",
    "                data_spo[i][2] = 1\n",
    "            elif  post < 5:\n",
    "                data_spo[i][2] = 2\n",
    "            elif post < 10:\n",
    "                data_spo[i][2] = 3\n",
    "            else:\n",
    "                data_spo[i][2] = 4\n",
    "     \n",
    "    # Take away 10% which should be used for the validation\n",
    "    idx_val=np.random.choice(data_pos.shape[0], int(data_pos.shape[0]*0.10), replace=False)\n",
    "\n",
    "    valid_data_pos = data_pos[idx_val,:]\n",
    "    train_data_pos = np.delete(data_pos, idx_val, axis=0)\n",
    "    \n",
    "    valid_data_spo = data_spo[idx_val,:]\n",
    "    train_data_spo = np.delete(data_spo, idx_val, axis=0)\n",
    "    \n",
    "    classes = np.zeros(5)\n",
    "    for t in train_data_pos:\n",
    "        pos = t[2]\n",
    "        classes[pos] += 1\n",
    "    print(classes)\n",
    "\n",
    "    return train_data_pos, valid_data_pos, train_data_spo, valid_data_spo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13774.  3330.  1374.  1650.  3831.]\n",
      "Test count =  2662\n"
     ]
    }
   ],
   "source": [
    "train_data_pos, valid_data_pos, train_data_spo, valid_data_spo = get_train_valid_data(logfile)#graph_type='SPO'\n",
    "\n",
    "train_data = train_data_pos\n",
    "valid_data = valid_data_pos\n",
    "\n",
    "#train_data = train_data_spo\n",
    "#valid_data = valid_data_spo\n",
    "\n",
    "testCount = 0\n",
    "for x,y in zip(valid_data_pos, valid_data_spo):\n",
    "    testCount += 1\n",
    "    if x[0] != y[0]:\n",
    "        print(\"FATAL\")\n",
    "print (\"Test count = \", testCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a simple logistic regression model using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 50#100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "n_input = dims * 2\n",
    "n_classes = 5\n",
    "n_hidden_1 = 256 # n neurons first layer\n",
    "n_hidden_2 = 256 # n neurons second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the input queues\n",
    "t_emb_e = tf.constant(emb_e)\n",
    "t_emb_r = tf.constant(emb_r)\n",
    "\n",
    "# Initialize the training data\n",
    "t = tf.constant(train_data)\n",
    "ds = tf.data.Dataset.from_tensor_slices(t)\n",
    "ds = ds.shuffle(buffer_size=100)\n",
    "ds = ds.batch(batch_size)\n",
    "\n",
    "# Initialize the valid data\n",
    "t_valid = tf.constant(valid_data)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices(t_valid)\n",
    "ds_valid = ds_valid.batch(batch_size)\n",
    "\n",
    "\n",
    "#with tf.Session() as default_session:\n",
    "#    test = default_session.run([t_valid])\n",
    "    #print(test[0][55])\n",
    "#    print(len(test[0]))\n",
    "\n",
    "#iter = ds.make_initializable_iterator()\n",
    "iter = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "el = iter.get_next()\n",
    "\n",
    "# Lookup embeddings for the inputs\n",
    "rel, ent, y = tf.split(el, num_or_size_splits=3, axis=1)\n",
    "rel = tf.reshape(rel, shape=[-1]) # Flatten the tensor\n",
    "ent = tf.reshape(ent, shape=[-1]) # Flatten the tensor\n",
    "rel_emb = tf.nn.embedding_lookup(t_emb_r, rel)\n",
    "ent_emb = tf.nn.embedding_lookup(t_emb_e, ent)\n",
    "inp = tf.concat([rel_emb, ent_emb], axis=1)\n",
    "\n",
    "#Process the labels\n",
    "y = tf.reshape(y, shape=[-1])\n",
    "y_hot = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "layer_1 = tf.add(tf.matmul(inp, weights['h1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "pred = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=y_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New operator to perform the predictions\n",
    "predictions = tf.nn.softmax(out_layer, name='predictions')\n",
    "# Compare the softmax with the actual values\n",
    "pred_indices = tf.argmax(predictions, axis=1)\n",
    "res = tf.equal(pred_indices, y)\n",
    "res = tf.cast(res, tf.int32)\n",
    "sres = tf.reduce_sum(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0  Loss=538.971007 Time=0.8379sec\n",
      "Correct predictions=  1146\n",
      "*** MY correct Pred=  1146\n",
      "Train epoch 1  Loss=231.894607 Time=0.5200sec\n",
      "Train epoch 2  Loss=198.348340 Time=0.5075sec\n",
      "Train epoch 3  Loss=133.655735 Time=0.5134sec\n",
      "Train epoch 4  Loss=81.048480 Time=0.5071sec\n",
      "Train epoch 5  Loss=58.802704 Time=0.5102sec\n",
      "Train epoch 6  Loss=48.161137 Time=0.5105sec\n",
      "Train epoch 7  Loss=35.214795 Time=0.5042sec\n",
      "Train epoch 8  Loss=27.850070 Time=0.5067sec\n",
      "Train epoch 9  Loss=23.328911 Time=0.5171sec\n",
      "Train epoch 10  Loss=17.664958 Time=0.5004sec\n",
      "Correct predictions=  1344\n",
      "*** MY correct Pred=  1344\n",
      "Train epoch 11  Loss=13.979755 Time=0.4998sec\n",
      "Train epoch 12  Loss=11.956160 Time=0.5044sec\n",
      "Train epoch 13  Loss=14.336175 Time=0.4982sec\n",
      "Train epoch 14  Loss=12.539554 Time=0.5173sec\n",
      "Train epoch 15  Loss=10.326099 Time=0.5025sec\n",
      "Train epoch 16  Loss=8.199671 Time=0.5038sec\n",
      "Train epoch 17  Loss=7.205559 Time=0.5187sec\n",
      "Train epoch 18  Loss=7.666417 Time=0.6668sec\n",
      "Train epoch 19  Loss=6.539701 Time=0.6912sec\n",
      "Train epoch 20  Loss=5.212174 Time=0.6619sec\n",
      "Correct predictions=  1277\n",
      "*** MY correct Pred=  1277\n",
      "Train epoch 21  Loss=5.111365 Time=0.5233sec\n",
      "Train epoch 22  Loss=4.922413 Time=0.4949sec\n",
      "Train epoch 23  Loss=5.257788 Time=0.5020sec\n",
      "Train epoch 24  Loss=4.634952 Time=0.5037sec\n",
      "Train epoch 25  Loss=5.515904 Time=0.5053sec\n",
      "Train epoch 26  Loss=3.971701 Time=0.4950sec\n",
      "Train epoch 27  Loss=3.432573 Time=0.5008sec\n",
      "Train epoch 28  Loss=3.971270 Time=0.5042sec\n",
      "Train epoch 29  Loss=4.311828 Time=0.5124sec\n",
      "Train epoch 30  Loss=4.244426 Time=0.4979sec\n",
      "Correct predictions=  1280\n",
      "*** MY correct Pred=  1280\n",
      "Train epoch 31  Loss=3.583774 Time=0.5043sec\n",
      "Train epoch 32  Loss=3.190721 Time=0.4996sec\n",
      "Train epoch 33  Loss=5.054337 Time=0.4952sec\n",
      "Train epoch 34  Loss=5.397116 Time=0.4967sec\n",
      "Train epoch 35  Loss=4.921498 Time=0.5106sec\n",
      "Train epoch 36  Loss=5.805112 Time=0.4985sec\n",
      "Train epoch 37  Loss=5.294245 Time=0.4986sec\n",
      "Train epoch 38  Loss=4.579345 Time=0.5057sec\n",
      "Train epoch 39  Loss=3.004930 Time=0.5048sec\n",
      "Train epoch 40  Loss=2.908564 Time=0.5027sec\n",
      "Correct predictions=  1417\n",
      "*** MY correct Pred=  1417\n",
      "Train epoch 41  Loss=3.022839 Time=0.5342sec\n",
      "Train epoch 42  Loss=2.458925 Time=0.7052sec\n",
      "Train epoch 43  Loss=1.977224 Time=0.6484sec\n",
      "Train epoch 44  Loss=1.930569 Time=0.6820sec\n",
      "Train epoch 45  Loss=1.951652 Time=0.6935sec\n",
      "Train epoch 46  Loss=2.105443 Time=0.6826sec\n",
      "Train epoch 47  Loss=2.184369 Time=0.6713sec\n",
      "Train epoch 48  Loss=2.983209 Time=0.6693sec\n",
      "Train epoch 49  Loss=3.145052 Time=0.6748sec\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_input_initializer = iter.make_initializer(ds)\n",
    "valid_input_initializer = iter.make_initializer(ds_valid)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init])\n",
    "    for epoch in range(training_epochs):\n",
    "        start = time.time()\n",
    "        sess.run(train_input_initializer)\n",
    "        # Loop over all batches\n",
    "        avg_loss = 0\n",
    "        num_batch = 0\n",
    "        while True:\n",
    "            try:\n",
    "                loss = sess.run([loss_op, train_op])                \n",
    "                num_batch += 1\n",
    "                avg_loss += loss[0]\n",
    "            except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                break\n",
    "            except e:\n",
    "                print(e)\n",
    "                break\n",
    "        print('Train epoch', epoch, \" Loss={:.6f}\".format(avg_loss / num_batch), \"Time={:.4f}sec\".format(time.time() - start))\n",
    "        if epoch % 10 == 0:\n",
    "            # Test the performance on the valid dataset\n",
    "            sess.run(valid_input_initializer)\n",
    "            \n",
    "            num_batch = 0\n",
    "            correct = 0\n",
    "            myCorrect = 0\n",
    "            predLog = open('fb15k-predictions.pos.log', 'w')\n",
    "            while True:\n",
    "                try:\n",
    "                    p = sess.run([sres, pred_indices, y, rel, ent, t_valid])                \n",
    "                    correct += p[0]\n",
    "                    \n",
    "                    #print(\"# batches = \", num_batch)\n",
    "                    #print(\"p = \", len(p))\n",
    "                    #out = pred_indices.eval()\n",
    "                    #print(\"predicted indices: \", p[1])\n",
    "                    #expected = y.eval()\n",
    "                    #print(\"Expected indices : \", p[2])\n",
    "                    #print(\"correct predictions = \", p[0])\n",
    "                    #print('*'*80)\n",
    "                    row = \"\"\n",
    "                    count = 0\n",
    "                    for r,e,k in zip(p[3], p[4], p[1]):\n",
    "                        row += str(r) + \" \" + str(e) + \" \" + str(k) + \"\\n\"\n",
    "                        #print(str(r) + \" \" + str(e) + \" \" + str(k))\n",
    "                        #print(p[5][100*num_batch + count])\n",
    "                        if int(k) == p[5][100*num_batch + count][2]:\n",
    "                            myCorrect += 1\n",
    "                        count +=1\n",
    "                    predLog.write(row)\n",
    "                    \n",
    "                    num_batch += 1\n",
    "                    #print(count, \" rows written\")\n",
    "                    \n",
    "                except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                    break\n",
    "                except e:\n",
    "                    print(e)\n",
    "            print(\"Correct predictions= \", correct)\n",
    "            print(\"*** MY correct Pred= \", myCorrect)\n",
    "    predLog.close()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uji300/karma/trident/scripts\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the input queues\n",
    "t_emb_e = tf.constant(emb_e)\n",
    "t_emb_r = tf.constant(emb_r)\n",
    "\n",
    "# Initialize the training data\n",
    "t = tf.constant(train_data_spo)\n",
    "ds = tf.data.Dataset.from_tensor_slices(t)\n",
    "ds = ds.shuffle(buffer_size=100)\n",
    "ds = ds.batch(batch_size)\n",
    "\n",
    "# Initialize the valid data\n",
    "t_valid = tf.constant(valid_data_spo)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices(t_valid)\n",
    "ds_valid = ds_valid.batch(batch_size)\n",
    "\n",
    "\n",
    "#with tf.Session() as default_session:\n",
    "#    test = default_session.run([t_valid])\n",
    "    #print(test[0][55])\n",
    "#    print(len(test[0]))\n",
    "\n",
    "#iter = ds.make_initializable_iterator()\n",
    "iter = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "el = iter.get_next()\n",
    "\n",
    "# Lookup embeddings for the inputs\n",
    "rel, ent, y = tf.split(el, num_or_size_splits=3, axis=1)\n",
    "rel = tf.reshape(rel, shape=[-1]) # Flatten the tensor\n",
    "ent = tf.reshape(ent, shape=[-1]) # Flatten the tensor\n",
    "rel_emb = tf.nn.embedding_lookup(t_emb_r, rel)\n",
    "ent_emb = tf.nn.embedding_lookup(t_emb_e, ent)\n",
    "inp = tf.concat([rel_emb, ent_emb], axis=1)\n",
    "\n",
    "#Process the labels\n",
    "y = tf.reshape(y, shape=[-1])\n",
    "y_hot = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "layer_1 = tf.add(tf.matmul(inp, weights['h1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "pred = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=y_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New operator to perform the predictions\n",
    "predictions = tf.nn.softmax(out_layer, name='predictions')\n",
    "# Compare the softmax with the actual values\n",
    "pred_indices = tf.argmax(predictions, axis=1)\n",
    "res = tf.equal(pred_indices, y)\n",
    "res = tf.cast(res, tf.int32)\n",
    "sres = tf.reduce_sum(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0  Loss=376.057455 Time=0.8333sec\n",
      "Correct predictions=  1269\n",
      "*** MY correct Pred=  1269\n",
      "Train epoch 1  Loss=311.153680 Time=0.5860sec\n",
      "Train epoch 2  Loss=202.498197 Time=0.6498sec\n",
      "Train epoch 3  Loss=130.024789 Time=0.6707sec\n",
      "Train epoch 4  Loss=84.181611 Time=0.6818sec\n",
      "Train epoch 5  Loss=55.407388 Time=0.6971sec\n",
      "Train epoch 6  Loss=41.994379 Time=0.6820sec\n",
      "Train epoch 7  Loss=37.773584 Time=0.6692sec\n",
      "Train epoch 8  Loss=24.142956 Time=0.6871sec\n",
      "Train epoch 9  Loss=22.431698 Time=0.5913sec\n",
      "Train epoch 10  Loss=17.931837 Time=0.5079sec\n",
      "Correct predictions=  1179\n",
      "*** MY correct Pred=  1179\n",
      "Train epoch 11  Loss=15.196494 Time=0.5050sec\n",
      "Train epoch 12  Loss=14.133160 Time=0.5085sec\n",
      "Train epoch 13  Loss=12.538621 Time=0.5018sec\n",
      "Train epoch 14  Loss=12.906458 Time=0.5071sec\n",
      "Train epoch 15  Loss=9.693669 Time=0.5057sec\n",
      "Train epoch 16  Loss=9.497685 Time=0.5058sec\n",
      "Train epoch 17  Loss=8.202697 Time=0.5020sec\n",
      "Train epoch 18  Loss=6.579006 Time=0.4991sec\n",
      "Train epoch 19  Loss=7.657306 Time=0.5197sec\n",
      "Train epoch 20  Loss=6.918101 Time=0.5007sec\n",
      "Correct predictions=  1401\n",
      "*** MY correct Pred=  1401\n",
      "Train epoch 21  Loss=6.437900 Time=0.5033sec\n",
      "Train epoch 22  Loss=5.934192 Time=0.5010sec\n",
      "Train epoch 23  Loss=4.965147 Time=0.4983sec\n",
      "Train epoch 24  Loss=4.673447 Time=0.4899sec\n",
      "Train epoch 25  Loss=5.085721 Time=0.5052sec\n",
      "Train epoch 26  Loss=4.980064 Time=0.4909sec\n",
      "Train epoch 27  Loss=4.384863 Time=0.4929sec\n",
      "Train epoch 28  Loss=3.659310 Time=0.5065sec\n",
      "Train epoch 29  Loss=3.827548 Time=0.5034sec\n",
      "Train epoch 30  Loss=4.032963 Time=0.4871sec\n",
      "Correct predictions=  1304\n",
      "*** MY correct Pred=  1304\n",
      "Train epoch 31  Loss=5.414838 Time=0.4991sec\n",
      "Train epoch 32  Loss=4.765473 Time=0.5067sec\n",
      "Train epoch 33  Loss=5.289219 Time=0.5128sec\n",
      "Train epoch 34  Loss=4.017385 Time=0.5019sec\n",
      "Train epoch 35  Loss=6.099343 Time=0.5002sec\n",
      "Train epoch 36  Loss=3.869363 Time=0.5058sec\n",
      "Train epoch 37  Loss=3.265270 Time=0.4997sec\n",
      "Train epoch 38  Loss=3.710463 Time=0.4999sec\n",
      "Train epoch 39  Loss=3.870245 Time=0.4961sec\n",
      "Train epoch 40  Loss=4.151907 Time=0.4912sec\n",
      "Correct predictions=  1374\n",
      "*** MY correct Pred=  1374\n",
      "Train epoch 41  Loss=3.010192 Time=0.5084sec\n",
      "Train epoch 42  Loss=2.996180 Time=0.4963sec\n",
      "Train epoch 43  Loss=2.252080 Time=0.4976sec\n",
      "Train epoch 44  Loss=2.027662 Time=0.5040sec\n",
      "Train epoch 45  Loss=1.754225 Time=0.5065sec\n",
      "Train epoch 46  Loss=1.515941 Time=0.4983sec\n",
      "Train epoch 47  Loss=1.525654 Time=0.5135sec\n",
      "Train epoch 48  Loss=1.461660 Time=0.5074sec\n",
      "Train epoch 49  Loss=1.459111 Time=0.4972sec\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_input_initializer = iter.make_initializer(ds)\n",
    "valid_input_initializer = iter.make_initializer(ds_valid)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init])\n",
    "    for epoch in range(training_epochs):\n",
    "        start = time.time()\n",
    "        sess.run(train_input_initializer)\n",
    "        # Loop over all batches\n",
    "        avg_loss = 0\n",
    "        num_batch = 0\n",
    "        while True:\n",
    "            try:\n",
    "                loss = sess.run([loss_op, train_op])                \n",
    "                num_batch += 1\n",
    "                avg_loss += loss[0]\n",
    "            except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                break\n",
    "            except e:\n",
    "                print(e)\n",
    "                break\n",
    "        print('Train epoch', epoch, \" Loss={:.6f}\".format(avg_loss / num_batch), \"Time={:.4f}sec\".format(time.time() - start))\n",
    "        if epoch % 10 == 0:\n",
    "            # Test the performance on the valid dataset\n",
    "            sess.run(valid_input_initializer)\n",
    "            \n",
    "            num_batch = 0\n",
    "            correct = 0\n",
    "            myCorrect = 0\n",
    "            predLog = open('fb15k-predictions.spo.log', 'w')\n",
    "            while True:\n",
    "                try:\n",
    "                    p = sess.run([sres, pred_indices, y, rel, ent, t_valid])                \n",
    "                    correct += p[0]\n",
    "                    \n",
    "                    #print(\"# batches = \", num_batch)\n",
    "                    #print(\"p = \", len(p))\n",
    "                    #out = pred_indices.eval()\n",
    "                    #print(\"predicted indices: \", p[1])\n",
    "                    #expected = y.eval()\n",
    "                    #print(\"Expected indices : \", p[2])\n",
    "                    #print(\"correct predictions = \", p[0])\n",
    "                    #print('*'*80)\n",
    "                    row = \"\"\n",
    "                    count = 0\n",
    "                    for r,e,k in zip(p[3], p[4], p[1]):\n",
    "                        row += str(r) + \" \" + str(e) + \" \" + str(k) + \"\\n\"\n",
    "                        #print(str(r) + \" \" + str(e) + \" \" + str(k))\n",
    "                        #print(p[5][100*num_batch + count])\n",
    "                        if int(k) == p[5][100*num_batch + count][2]:\n",
    "                            myCorrect += 1\n",
    "                        count +=1\n",
    "                    predLog.write(row)\n",
    "                    \n",
    "                    num_batch += 1\n",
    "                    #print(count, \" rows written\")\n",
    "                    \n",
    "                except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                    break\n",
    "                except e:\n",
    "                    print(e)\n",
    "            print(\"Correct predictions= \", correct)\n",
    "            print(\"*** MY correct Pred= \", myCorrect)\n",
    "    predLog.close()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
