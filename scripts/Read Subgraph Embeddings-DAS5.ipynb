{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import array\n",
    "import time\n",
    "\n",
    "SubgraphMeta = collections.namedtuple('SubgraphMeta', 'typ rel ent siz')\n",
    "Subgraph = collections.namedtuple('Subgraph', 'emb meta')\n",
    "Result = collections.namedtuple('Result', 'query posh post')\n",
    "   \n",
    "inputfile = '/var/scratch/uji300/kgemb/fb15k_avgsubgraphs.bin'\n",
    "logfile = '/var/scratch/uji300/kgemb/fb15K_subgraphs_valid.txt'\n",
    "emb_meta_e = '/var/scratch/uji300/kgemb/models/fb15k/best-model/E-meta'\n",
    "emb_e_path = '/var/scratch/uji300/kgemb/models/fb15k/best-model/E.0'\n",
    "emb_meta_r = '/var/scratch/uji300/kgemb/models/fb15k/best-model/R-meta'\n",
    "emb_r_path = '/var/scratch/uji300/kgemb/models/fb15k/best-model/R.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the file with the subgraph embeddings produced from Trident (for now I don't need them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subgraphs_meta = []\n",
    "embeddings = []\n",
    "with open(inputfile, 'rb') as fin:\n",
    "    b_nsubgraphs = fin.read(8)\n",
    "    nsubgraphs = int.from_bytes(b_nsubgraphs, byteorder='little', signed=False)\n",
    "    for i in range(nsubgraphs):\n",
    "        line = fin.read(25)\n",
    "        typ = line[0]\n",
    "        rel = int.from_bytes(line[1:9], byteorder='little', signed=False)\n",
    "        ent = int.from_bytes(line[9:17], byteorder='little', signed=False)\n",
    "        siz = int.from_bytes(line[17:], byteorder='little', signed=False)        \n",
    "        sg = SubgraphMeta(typ=typ, ent=ent, siz=siz, rel=rel)\n",
    "        subgraphs_meta.append(sg)\n",
    "    # Load the average embeddings\n",
    "    emb_meta = fin.read(10)\n",
    "    dims = int.from_bytes(emb_meta[:2], byteorder='big', signed=False)\n",
    "    mincard = int.from_bytes(emb_meta[2:], byteorder='big', signed=False)\n",
    "    for i in range(nsubgraphs):\n",
    "        b_emb = fin.read(dims * 8)\n",
    "        emb = np.frombuffer(b_emb, dtype=np.float64)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "subgraphs = []\n",
    "for i in range(nsubgraphs):\n",
    "    subgraphs.append(Subgraph(emb=embeddings[i], meta=subgraphs_meta[i]))\n",
    "subgraphs_meta = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(meta, e_path):\n",
    "    batch_size = 0\n",
    "    dim = 0\n",
    "    n = 0\n",
    "    with open(meta, 'rb') as fmeta:\n",
    "        raw = fmeta.read(10)\n",
    "        batch_size = int.from_bytes(raw[:4], byteorder='little', signed=False)\n",
    "        n = int.from_bytes(raw[4:8], byteorder='little', signed=False)\n",
    "        dim = int.from_bytes(raw[8:], byteorder='little', signed=False)\n",
    "    e = np.zeros(shape=(n,dim))\n",
    "    with open(e_path, 'rb') as fin:\n",
    "        for i in range(n):\n",
    "            line = fin.read(8 + dim * 8)            \n",
    "            emb = np.frombuffer(line[8:], dtype=np.float64)\n",
    "            e[i] = emb\n",
    "    return e\n",
    "emb_e = load_embeddings(emb_meta_e, emb_e_path)\n",
    "emb_r = load_embeddings(emb_meta_r, emb_r_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_valid_data(logfile, graph_type='POS'):\n",
    "    results = []\n",
    "    with open(logfile, 'rt') as fin:\n",
    "        header = fin.readline()\n",
    "        for l in fin:\n",
    "            tkns = l.split('\\t')\n",
    "            query = tkns[0]\n",
    "            pos_answer_subgraph_head = int(tkns[1])\n",
    "            pos_answer_subgraph_tail = int(tkns[2])\n",
    "            results.append(Result(query, pos_answer_subgraph_head, pos_answer_subgraph_tail))\n",
    "    # Create the training data\n",
    "    data_pos = np.zeros((len(results), 3), dtype=np.int)\n",
    "    data_spo = np.zeros((len(results), 3), dtype=np.int)\n",
    "    raw_data_pos = np.zeros((len(results), 3), dtype=np.int)\n",
    "    raw_data_spo = np.zeros((len(results), 3), dtype=np.int)\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        query = results[i].query\n",
    "        tkns = query.split(' ')\n",
    "        h = int(tkns[0])\n",
    "        r = int(tkns[1])\n",
    "        t = int(tkns[2])\n",
    "        \n",
    "        \n",
    "        data_pos[i][0] = r\n",
    "        data_pos[i][1] = t\n",
    "        raw_data_pos[i][0] = r\n",
    "        raw_data_pos[i][1] = t\n",
    "        raw_data_pos[i][2] = results[i].posh + 1\n",
    "        \n",
    "        data_spo[i][0] = r\n",
    "        data_spo[i][1] = h\n",
    "        raw_data_spo[i][0] = r\n",
    "        raw_data_spo[i][1] = h\n",
    "        raw_data_spo[i][2] = results[i].post + 1\n",
    "        \n",
    "        posh = results[i].posh\n",
    "        post = results[i].post\n",
    "        \n",
    "        if posh > 0:\n",
    "            if posh < 3:\n",
    "                data_pos[i][2] = 1\n",
    "            elif  posh < 5:\n",
    "                data_pos[i][2] = 2\n",
    "            elif posh < 10:\n",
    "                data_pos[i][2] = 3\n",
    "            else:\n",
    "                data_pos[i][2] = 4\n",
    "        \n",
    "        if post > 0:\n",
    "            if post < 3:\n",
    "                data_spo[i][2] = 1\n",
    "            elif  post < 5:\n",
    "                data_spo[i][2] = 2\n",
    "            elif post < 10:\n",
    "                data_spo[i][2] = 3\n",
    "            else:\n",
    "                data_spo[i][2] = 4\n",
    "     \n",
    "    # Take away 10% which should be used for the validation\n",
    "    idx_val=np.random.choice(data_pos.shape[0], int(data_pos.shape[0]*0.10), replace=False)\n",
    "\n",
    "    valid_data_pos = data_pos[idx_val,:]\n",
    "    train_data_pos = np.delete(data_pos, idx_val, axis=0)\n",
    "    valid_raw_data_pos = raw_data_pos[idx_val, :]\n",
    "    train_raw_data_pos = np.delete(raw_data_pos, idx_val, axis=0)\n",
    "    \n",
    "    valid_data_spo = data_spo[idx_val,:]\n",
    "    train_data_spo = np.delete(data_spo, idx_val, axis=0)\n",
    "    valid_raw_data_spo = raw_data_spo[idx_val, :]\n",
    "    train_raw_data_spo = np.delete(raw_data_spo, idx_val, axis=0)\n",
    "    \n",
    "    classes = np.zeros(5)\n",
    "    for t in train_data_pos:\n",
    "        pos = t[2]\n",
    "        classes[pos] += 1\n",
    "    print(classes)\n",
    "\n",
    "    return train_data_pos, valid_data_pos, train_data_spo, valid_data_spo, train_raw_data_pos, valid_raw_data_pos, train_raw_data_spo, valid_raw_data_spo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13736.  3342.  1372.  1667.  3842.]\n",
      "Test count =  2662\n"
     ]
    }
   ],
   "source": [
    "train_data_pos, valid_data_pos, train_data_spo, valid_data_spo, train_raw_data_pos, valid_raw_data_pos, train_raw_data_spo, valid_raw_data_spo = get_train_valid_data(logfile)#graph_type='SPO'\n",
    "\n",
    "train_data = train_data_pos\n",
    "valid_data = valid_data_pos\n",
    "\n",
    "#train_data = train_data_spo\n",
    "#valid_data = valid_data_spo\n",
    "\n",
    "testCount = 0\n",
    "for x,y in zip(valid_data_pos, valid_data_spo):\n",
    "    testCount += 1\n",
    "    if x[0] != y[0]:\n",
    "        print(\"FATAL\")\n",
    "print (\"Test count = \", testCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For all training samples, find the average of top K for each relation\n",
    "# triple contains the array of (r, e, k)\n",
    "# where\n",
    "# r is a relation\n",
    "# e is an entity\n",
    "# k is the topk value\n",
    "def find_average_topk_for_relations(triples):\n",
    "    relstats = {}\n",
    "    for sample in triples:\n",
    "        r = sample[0]\n",
    "        e = sample[1]\n",
    "        k = sample[2] \n",
    "\n",
    "        if r not in relstats.keys():\n",
    "            if k == 0: # Means not found\n",
    "                relstats[r] = (1, 0, k)\n",
    "            else:\n",
    "                relstats[r] = (1, 1, k)\n",
    "        else:\n",
    "            n, hits, sumK = relstats[r]\n",
    "            if k == 0:\n",
    "                relstats[r] = (n+1, hits, sumK + k)\n",
    "            else:\n",
    "                relstats[r] = (n+1, hits+1, sumK + k)\n",
    "\n",
    "    for key,value in relstats.items():\n",
    "        if value[1] == 0: # No hits\n",
    "            relstats[key] = (value[0], value[1], value[2], -1)\n",
    "        else:\n",
    "            relstats[key] = (value[0], value[1], value[2], int(round(value[2]/value[1])))\n",
    "        \n",
    "    return relstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relstats_spo = find_average_topk_for_relations(train_raw_data_spo)\n",
    "relstats_pos = find_average_topk_for_relations(train_raw_data_pos)\n",
    "\n",
    "output = open(\"fb15k-per-relation-K.log\", 'w')\n",
    "\n",
    "for key in relstats_pos.keys():\n",
    "    row = str(key) + \" \" + str(relstats_pos[key][3]) + \" \"\n",
    "    if key in relstats_spo.keys():\n",
    "        row += str(relstats_spo[key][3])\n",
    "    else:\n",
    "        row += \"-1\"\n",
    "    row += \"\\n\"\n",
    "    output.write(row)\n",
    "\n",
    "for key in relstats_spo.keys():\n",
    "    if key not in relstats_pos.keys():\n",
    "        row = str(key) + \" \" + \"-1\" + \" \" + str(relstats_spo[key][3]) + \"\\n\"\n",
    "        output.write(row)\n",
    "        \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a simple logistic regression model using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 50#100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "n_input = dims * 2\n",
    "n_classes = 5\n",
    "n_hidden_1 = 256 # n neurons first layer\n",
    "n_hidden_2 = 256 # n neurons second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the input queues\n",
    "t_emb_e = tf.constant(emb_e)\n",
    "t_emb_r = tf.constant(emb_r)\n",
    "\n",
    "# Initialize the training data\n",
    "t = tf.constant(train_data)\n",
    "ds = tf.data.Dataset.from_tensor_slices(t)\n",
    "ds = ds.shuffle(buffer_size=100)\n",
    "ds = ds.batch(batch_size)\n",
    "\n",
    "# Initialize the valid data\n",
    "t_valid = tf.constant(valid_data)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices(t_valid)\n",
    "ds_valid = ds_valid.batch(batch_size)\n",
    "\n",
    "\n",
    "#with tf.Session() as default_session:\n",
    "#    test = default_session.run([t_valid])\n",
    "    #print(test[0][55])\n",
    "#    print(len(test[0]))\n",
    "\n",
    "#iter = ds.make_initializable_iterator()\n",
    "iter = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "el = iter.get_next()\n",
    "\n",
    "# Lookup embeddings for the inputs\n",
    "rel, ent, y = tf.split(el, num_or_size_splits=3, axis=1)\n",
    "rel = tf.reshape(rel, shape=[-1]) # Flatten the tensor\n",
    "ent = tf.reshape(ent, shape=[-1]) # Flatten the tensor\n",
    "rel_emb = tf.nn.embedding_lookup(t_emb_r, rel)\n",
    "ent_emb = tf.nn.embedding_lookup(t_emb_e, ent)\n",
    "inp = tf.concat([rel_emb, ent_emb], axis=1)\n",
    "\n",
    "#Process the labels\n",
    "y = tf.reshape(y, shape=[-1])\n",
    "y_hot = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "layer_1 = tf.add(tf.matmul(inp, weights['h1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "pred = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=y_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New operator to perform the predictions\n",
    "predictions = tf.nn.softmax(out_layer, name='predictions')\n",
    "# Compare the softmax with the actual values\n",
    "pred_indices = tf.argmax(predictions, axis=1)\n",
    "res = tf.equal(pred_indices, y)\n",
    "res = tf.cast(res, tf.int32)\n",
    "sres = tf.reduce_sum(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0  Loss=485.048033 Time=0.8137sec\n",
      "Correct predictions=  1180\n",
      "*** MY correct Pred=  1180\n",
      "Train epoch 1  Loss=199.149526 Time=0.6888sec\n",
      "Train epoch 2  Loss=157.289078 Time=0.6854sec\n",
      "Train epoch 3  Loss=99.203002 Time=0.6797sec\n",
      "Train epoch 4  Loss=66.874956 Time=0.6246sec\n",
      "Train epoch 5  Loss=50.318858 Time=0.5327sec\n",
      "Train epoch 6  Loss=40.876943 Time=0.5109sec\n",
      "Train epoch 7  Loss=37.985119 Time=0.5258sec\n",
      "Train epoch 8  Loss=30.472944 Time=0.5216sec\n",
      "Train epoch 9  Loss=22.169807 Time=0.5134sec\n",
      "Train epoch 10  Loss=21.503333 Time=0.5139sec\n",
      "Correct predictions=  1274\n",
      "*** MY correct Pred=  1274\n",
      "Train epoch 11  Loss=17.825708 Time=0.5003sec\n",
      "Train epoch 12  Loss=12.842469 Time=0.5267sec\n",
      "Train epoch 13  Loss=10.710012 Time=0.5125sec\n",
      "Train epoch 14  Loss=11.300973 Time=0.5040sec\n",
      "Train epoch 15  Loss=8.359526 Time=0.5088sec\n",
      "Train epoch 16  Loss=7.993140 Time=0.5253sec\n",
      "Train epoch 17  Loss=7.137404 Time=0.5273sec\n",
      "Train epoch 18  Loss=6.980509 Time=0.5051sec\n",
      "Train epoch 19  Loss=6.540934 Time=0.5121sec\n",
      "Train epoch 20  Loss=5.348445 Time=0.4969sec\n",
      "Correct predictions=  1241\n",
      "*** MY correct Pred=  1241\n",
      "Train epoch 21  Loss=4.959142 Time=0.4970sec\n",
      "Train epoch 22  Loss=5.508521 Time=0.4949sec\n",
      "Train epoch 23  Loss=5.798399 Time=0.4867sec\n",
      "Train epoch 24  Loss=6.125332 Time=0.4898sec\n",
      "Train epoch 25  Loss=6.207223 Time=0.5010sec\n",
      "Train epoch 26  Loss=4.911559 Time=0.5068sec\n",
      "Train epoch 27  Loss=3.990732 Time=0.4998sec\n",
      "Train epoch 28  Loss=3.411810 Time=0.5259sec\n",
      "Train epoch 29  Loss=3.326727 Time=0.5100sec\n",
      "Train epoch 30  Loss=3.078346 Time=0.4880sec\n",
      "Correct predictions=  1359\n",
      "*** MY correct Pred=  1359\n",
      "Train epoch 31  Loss=3.122295 Time=0.4966sec\n",
      "Train epoch 32  Loss=3.361879 Time=0.4858sec\n",
      "Train epoch 33  Loss=3.453737 Time=0.4841sec\n",
      "Train epoch 34  Loss=4.365921 Time=0.5004sec\n",
      "Train epoch 35  Loss=4.408309 Time=0.4872sec\n",
      "Train epoch 36  Loss=6.337583 Time=0.4959sec\n",
      "Train epoch 37  Loss=5.326747 Time=0.4908sec\n",
      "Train epoch 38  Loss=3.115891 Time=0.4918sec\n",
      "Train epoch 39  Loss=3.531269 Time=0.4851sec\n",
      "Train epoch 40  Loss=2.902986 Time=0.5085sec\n",
      "Correct predictions=  1458\n",
      "*** MY correct Pred=  1458\n",
      "Train epoch 41  Loss=2.317346 Time=0.4994sec\n",
      "Train epoch 42  Loss=2.530913 Time=0.4909sec\n",
      "Train epoch 43  Loss=2.245036 Time=0.5123sec\n",
      "Train epoch 44  Loss=3.061971 Time=0.4933sec\n",
      "Train epoch 45  Loss=5.182374 Time=0.5127sec\n",
      "Train epoch 46  Loss=4.389404 Time=0.5143sec\n",
      "Train epoch 47  Loss=2.354795 Time=0.5161sec\n",
      "Train epoch 48  Loss=2.051924 Time=0.5028sec\n",
      "Train epoch 49  Loss=2.010592 Time=0.4984sec\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_input_initializer = iter.make_initializer(ds)\n",
    "valid_input_initializer = iter.make_initializer(ds_valid)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init])\n",
    "    for epoch in range(training_epochs):\n",
    "        start = time.time()\n",
    "        sess.run(train_input_initializer)\n",
    "        # Loop over all batches\n",
    "        avg_loss = 0\n",
    "        num_batch = 0\n",
    "        while True:\n",
    "            try:\n",
    "                loss = sess.run([loss_op, train_op])                \n",
    "                num_batch += 1\n",
    "                avg_loss += loss[0]\n",
    "            except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                break\n",
    "            except e:\n",
    "                print(e)\n",
    "                break\n",
    "        print('Train epoch', epoch, \" Loss={:.6f}\".format(avg_loss / num_batch), \"Time={:.4f}sec\".format(time.time() - start))\n",
    "        if epoch % 10 == 0:\n",
    "            # Test the performance on the valid dataset\n",
    "            sess.run(valid_input_initializer)\n",
    "            \n",
    "            num_batch = 0\n",
    "            correct = 0\n",
    "            myCorrect = 0\n",
    "            predLog = open('fb15k-predictions.pos.log', 'w')\n",
    "            while True:\n",
    "                try:\n",
    "                    p = sess.run([sres, pred_indices, y, rel, ent, t_valid])                \n",
    "                    correct += p[0]\n",
    "                    \n",
    "                    #print(\"# batches = \", num_batch)\n",
    "                    #print(\"p = \", len(p))\n",
    "                    #out = pred_indices.eval()\n",
    "                    #print(\"predicted indices: \", p[1])\n",
    "                    #expected = y.eval()\n",
    "                    #print(\"Expected indices : \", p[2])\n",
    "                    #print(\"correct predictions = \", p[0])\n",
    "                    #print('*'*80)\n",
    "                    row = \"\"\n",
    "                    count = 0\n",
    "                    for r,e,k in zip(p[3], p[4], p[1]):\n",
    "                        row += str(r) + \" \" + str(e) + \" \" + str(k) + \"\\n\"\n",
    "                        #print(str(r) + \" \" + str(e) + \" \" + str(k))\n",
    "                        #print(p[5][100*num_batch + count])\n",
    "                        if int(k) == p[5][100*num_batch + count][2]:\n",
    "                            myCorrect += 1\n",
    "                        count +=1\n",
    "                    predLog.write(row)\n",
    "                    \n",
    "                    num_batch += 1\n",
    "                    #print(count, \" rows written\")\n",
    "                    \n",
    "                except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                    break\n",
    "                except e:\n",
    "                    print(e)\n",
    "            print(\"Correct predictions= \", correct)\n",
    "            print(\"*** MY correct Pred= \", myCorrect)\n",
    "    predLog.close()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uji300/karma/trident/scripts\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the input queues\n",
    "t_emb_e = tf.constant(emb_e)\n",
    "t_emb_r = tf.constant(emb_r)\n",
    "\n",
    "# Initialize the training data\n",
    "t = tf.constant(train_data_spo)\n",
    "ds = tf.data.Dataset.from_tensor_slices(t)\n",
    "ds = ds.shuffle(buffer_size=100)\n",
    "ds = ds.batch(batch_size)\n",
    "\n",
    "# Initialize the valid data\n",
    "t_valid = tf.constant(valid_data_spo)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices(t_valid)\n",
    "ds_valid = ds_valid.batch(batch_size)\n",
    "\n",
    "\n",
    "#with tf.Session() as default_session:\n",
    "#    test = default_session.run([t_valid])\n",
    "    #print(test[0][55])\n",
    "#    print(len(test[0]))\n",
    "\n",
    "#iter = ds.make_initializable_iterator()\n",
    "iter = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "el = iter.get_next()\n",
    "\n",
    "# Lookup embeddings for the inputs\n",
    "rel, ent, y = tf.split(el, num_or_size_splits=3, axis=1)\n",
    "rel = tf.reshape(rel, shape=[-1]) # Flatten the tensor\n",
    "ent = tf.reshape(ent, shape=[-1]) # Flatten the tensor\n",
    "rel_emb = tf.nn.embedding_lookup(t_emb_r, rel)\n",
    "ent_emb = tf.nn.embedding_lookup(t_emb_e, ent)\n",
    "inp = tf.concat([rel_emb, ent_emb], axis=1)\n",
    "\n",
    "#Process the labels\n",
    "y = tf.reshape(y, shape=[-1])\n",
    "y_hot = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "layer_1 = tf.add(tf.matmul(inp, weights['h1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "pred = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=y_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New operator to perform the predictions\n",
    "predictions = tf.nn.softmax(out_layer, name='predictions')\n",
    "# Compare the softmax with the actual values\n",
    "pred_indices = tf.argmax(predictions, axis=1)\n",
    "res = tf.equal(pred_indices, y)\n",
    "res = tf.cast(res, tf.int32)\n",
    "sres = tf.reduce_sum(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0  Loss=413.967736 Time=0.8008sec\n",
      "Correct predictions=  1363\n",
      "*** MY correct Pred=  1363\n",
      "Train epoch 1  Loss=287.016304 Time=0.5266sec\n",
      "Train epoch 2  Loss=157.410441 Time=0.5022sec\n",
      "Train epoch 3  Loss=126.735265 Time=0.5568sec\n",
      "Train epoch 4  Loss=101.224379 Time=0.5278sec\n",
      "Train epoch 5  Loss=52.106008 Time=0.5062sec\n",
      "Train epoch 6  Loss=34.139598 Time=0.5097sec\n",
      "Train epoch 7  Loss=27.246893 Time=0.5033sec\n",
      "Train epoch 8  Loss=24.640201 Time=0.4909sec\n",
      "Train epoch 9  Loss=19.253116 Time=0.5456sec\n",
      "Train epoch 10  Loss=17.090198 Time=0.4982sec\n",
      "Correct predictions=  1434\n",
      "*** MY correct Pred=  1434\n",
      "Train epoch 11  Loss=17.114777 Time=0.5289sec\n",
      "Train epoch 12  Loss=16.603428 Time=0.5038sec\n",
      "Train epoch 13  Loss=13.871548 Time=0.5091sec\n",
      "Train epoch 14  Loss=12.071080 Time=0.5298sec\n",
      "Train epoch 15  Loss=11.411668 Time=0.5183sec\n",
      "Train epoch 16  Loss=10.771891 Time=0.5165sec\n",
      "Train epoch 17  Loss=9.539417 Time=0.5194sec\n",
      "Train epoch 18  Loss=7.848466 Time=0.5275sec\n",
      "Train epoch 19  Loss=6.238831 Time=0.4986sec\n",
      "Train epoch 20  Loss=6.344982 Time=0.4994sec\n",
      "Correct predictions=  1336\n",
      "*** MY correct Pred=  1336\n",
      "Train epoch 21  Loss=5.806031 Time=0.5465sec\n",
      "Train epoch 22  Loss=5.465632 Time=0.4903sec\n",
      "Train epoch 23  Loss=4.826038 Time=0.4996sec\n",
      "Train epoch 24  Loss=3.878411 Time=0.5292sec\n",
      "Train epoch 25  Loss=4.012782 Time=0.5118sec\n",
      "Train epoch 26  Loss=3.893289 Time=0.5049sec\n",
      "Train epoch 27  Loss=3.377417 Time=0.5606sec\n",
      "Train epoch 28  Loss=3.691029 Time=0.4864sec\n",
      "Train epoch 29  Loss=3.579075 Time=0.4876sec\n",
      "Train epoch 30  Loss=3.882621 Time=0.5462sec\n",
      "Correct predictions=  1347\n",
      "*** MY correct Pred=  1347\n",
      "Train epoch 31  Loss=5.888203 Time=0.4983sec\n",
      "Train epoch 32  Loss=6.063368 Time=0.4899sec\n",
      "Train epoch 33  Loss=5.273738 Time=0.5597sec\n",
      "Train epoch 34  Loss=7.018386 Time=0.4917sec\n",
      "Train epoch 35  Loss=5.686985 Time=0.4967sec\n",
      "Train epoch 36  Loss=4.385251 Time=0.4882sec\n",
      "Train epoch 37  Loss=3.491995 Time=0.4809sec\n",
      "Train epoch 38  Loss=3.262793 Time=0.4856sec\n",
      "Train epoch 39  Loss=3.523867 Time=0.4755sec\n",
      "Train epoch 40  Loss=3.571981 Time=0.4945sec\n",
      "Correct predictions=  1349\n",
      "*** MY correct Pred=  1349\n",
      "Train epoch 41  Loss=2.796962 Time=0.4771sec\n",
      "Train epoch 42  Loss=3.120750 Time=0.4907sec\n",
      "Train epoch 43  Loss=3.121008 Time=0.4808sec\n",
      "Train epoch 44  Loss=2.759370 Time=0.4799sec\n",
      "Train epoch 45  Loss=2.462592 Time=0.4894sec\n",
      "Train epoch 46  Loss=1.950752 Time=0.4867sec\n",
      "Train epoch 47  Loss=1.809899 Time=0.4858sec\n",
      "Train epoch 48  Loss=1.989593 Time=0.4812sec\n",
      "Train epoch 49  Loss=2.269077 Time=0.4783sec\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_input_initializer = iter.make_initializer(ds)\n",
    "valid_input_initializer = iter.make_initializer(ds_valid)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init])\n",
    "    for epoch in range(training_epochs):\n",
    "        start = time.time()\n",
    "        sess.run(train_input_initializer)\n",
    "        # Loop over all batches\n",
    "        avg_loss = 0\n",
    "        num_batch = 0\n",
    "        while True:\n",
    "            try:\n",
    "                loss = sess.run([loss_op, train_op])                \n",
    "                num_batch += 1\n",
    "                avg_loss += loss[0]\n",
    "            except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                break\n",
    "            except e:\n",
    "                print(e)\n",
    "                break\n",
    "        print('Train epoch', epoch, \" Loss={:.6f}\".format(avg_loss / num_batch), \"Time={:.4f}sec\".format(time.time() - start))\n",
    "        if epoch % 10 == 0:\n",
    "            # Test the performance on the valid dataset\n",
    "            sess.run(valid_input_initializer)\n",
    "            \n",
    "            num_batch = 0\n",
    "            correct = 0\n",
    "            myCorrect = 0\n",
    "            predLog = open('fb15k-predictions.spo.log', 'w')\n",
    "            while True:\n",
    "                try:\n",
    "                    p = sess.run([sres, pred_indices, y, rel, ent, t_valid])                \n",
    "                    correct += p[0]\n",
    "                    \n",
    "                    #print(\"# batches = \", num_batch)\n",
    "                    #print(\"p = \", len(p))\n",
    "                    #out = pred_indices.eval()\n",
    "                    #print(\"predicted indices: \", p[1])\n",
    "                    #expected = y.eval()\n",
    "                    #print(\"Expected indices : \", p[2])\n",
    "                    #print(\"correct predictions = \", p[0])\n",
    "                    #print('*'*80)\n",
    "                    row = \"\"\n",
    "                    count = 0\n",
    "                    for r,e,k in zip(p[3], p[4], p[1]):\n",
    "                        row += str(r) + \" \" + str(e) + \" \" + str(k) + \"\\n\"\n",
    "                        #print(str(r) + \" \" + str(e) + \" \" + str(k))\n",
    "                        #print(p[5][100*num_batch + count])\n",
    "                        if int(k) == p[5][100*num_batch + count][2]:\n",
    "                            myCorrect += 1\n",
    "                        count +=1\n",
    "                    predLog.write(row)\n",
    "                    \n",
    "                    num_batch += 1\n",
    "                    #print(count, \" rows written\")\n",
    "                    \n",
    "                except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                    break\n",
    "                except e:\n",
    "                    print(e)\n",
    "            print(\"Correct predictions= \", correct)\n",
    "            print(\"*** MY correct Pred= \", myCorrect)\n",
    "    predLog.close()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = open('fb15k-predictions.pos.log', 'r').readlines()\n",
    "output = open(\"sorted_pos.txt\", 'w')\n",
    "\n",
    "for line in sorted(lines, key=lambda line: line.split()[0]):\n",
    "    output.write(line)\n",
    "\n",
    "output.close()\n",
    "\n",
    "lines = open('fb15k-predictions.spo.log', 'r').readlines()\n",
    "output = open(\"sorted_spo.txt\", 'w')\n",
    "\n",
    "for line in sorted(lines, key=lambda line: line.split()[0]):\n",
    "    output.write(line)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('sorted_pos.txt', 'r') as f1:\n",
    "    lines_pos = f1.readlines()\n",
    "\n",
    "with open('sorted_spo.txt', 'r') as f2:\n",
    "    lines_spo = f2.readlines()\n",
    "\n",
    "output = open(\"fb15k-dynamicK.log\", 'w')\n",
    "for line_pos, line_spo in zip(lines_pos, lines_spo):\n",
    "    tokens_pos = line_pos.split()\n",
    "    tokens_spo = line_spo.split()\n",
    "    rel_pos = tokens_pos[0]\n",
    "    ent_pos = tokens_pos[1]\n",
    "    k_pos   = tokens_pos[2]\n",
    "    \n",
    "    rel_spo = tokens_spo[0]\n",
    "    ent_spo = tokens_spo[1]\n",
    "    k_spo   = tokens_spo[2]\n",
    "    \n",
    "    if rel_pos != rel_spo:\n",
    "        print(\"FATAL !!!!!!!!!!!\")\n",
    "\n",
    "    log_line = str(ent_spo) + \" \" + str(rel_spo) + \" \" + str(ent_pos) + \" \" + str(k_pos) + \" \" + str(k_spo) + \"\\n\"\n",
    "    output.write(log_line)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
