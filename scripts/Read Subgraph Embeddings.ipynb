{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import array\n",
    "import time\n",
    "\n",
    "SubgraphMeta = collections.namedtuple('SubgraphMeta', 'typ rel ent siz')\n",
    "Subgraph = collections.namedtuple('Subgraph', 'emb meta')\n",
    "Result = collections.namedtuple('Result', 'query posh post')\n",
    "   \n",
    "inputfile = '/Users/jacopo/Desktop/kgemb/fb15k_avgsubgraphs.bin'\n",
    "logfile = '/Users/jacopo/Desktop/kgemb/fb15K_subgraphs_valid.txt'\n",
    "emb_meta_e = '/Users/jacopo/Desktop/kgemb/models/fb15k/best-model/E-meta'\n",
    "emb_e_path = '/Users/jacopo/Desktop/kgemb/models/fb15k/best-model/E.0'\n",
    "emb_meta_r = '/Users/jacopo/Desktop/kgemb/models/fb15k/best-model/R-meta'\n",
    "emb_r_path = '/Users/jacopo/Desktop/kgemb/models/fb15k/best-model/R.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the file with the subgraph embeddings produced from Trident (for now I don't need them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subgraphs_meta = []\n",
    "embeddings = []\n",
    "with open(inputfile, 'rb') as fin:\n",
    "    b_nsubgraphs = fin.read(8)\n",
    "    nsubgraphs = int.from_bytes(b_nsubgraphs, byteorder='little', signed=False)\n",
    "    for i in range(nsubgraphs):\n",
    "        line = fin.read(25)\n",
    "        typ = line[0]\n",
    "        rel = int.from_bytes(line[1:9], byteorder='little', signed=False)\n",
    "        ent = int.from_bytes(line[9:17], byteorder='little', signed=False)\n",
    "        siz = int.from_bytes(line[17:], byteorder='little', signed=False)        \n",
    "        sg = SubgraphMeta(typ=typ, ent=ent, siz=siz, rel=rel)\n",
    "        subgraphs_meta.append(sg)\n",
    "    # Load the average embeddings\n",
    "    emb_meta = fin.read(10)\n",
    "    dims = int.from_bytes(emb_meta[:2], byteorder='big', signed=False)\n",
    "    mincard = int.from_bytes(emb_meta[2:], byteorder='big', signed=False)\n",
    "    for i in range(nsubgraphs):\n",
    "        b_emb = fin.read(dims * 8)\n",
    "        emb = np.frombuffer(b_emb, dtype=np.float64)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "subgraphs = []\n",
    "for i in range(nsubgraphs):\n",
    "    subgraphs.append(Subgraph(emb=embeddings[i], meta=subgraphs_meta[i]))\n",
    "subgraphs_meta = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(meta, e_path):\n",
    "    batch_size = 0\n",
    "    dim = 0\n",
    "    n = 0\n",
    "    with open(meta, 'rb') as fmeta:\n",
    "        raw = fmeta.read(10)\n",
    "        batch_size = int.from_bytes(raw[:4], byteorder='little', signed=False)\n",
    "        n = int.from_bytes(raw[4:8], byteorder='little', signed=False)\n",
    "        dim = int.from_bytes(raw[8:], byteorder='little', signed=False)\n",
    "    e = np.zeros(shape=(n,dim))\n",
    "    with open(e_path, 'rb') as fin:\n",
    "        for i in range(n):\n",
    "            line = fin.read(8 + dim * 8)            \n",
    "            emb = np.frombuffer(line[8:], dtype=np.float64)\n",
    "            e[i] = emb\n",
    "    return e\n",
    "emb_e = load_embeddings(emb_meta_e, emb_e_path)\n",
    "emb_r = load_embeddings(emb_meta_r, emb_r_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13723.  3347.  1375.  1654.  3860.]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "with open(logfile, 'rt') as fin:\n",
    "    header = fin.readline()\n",
    "    for l in fin:\n",
    "        tkns = l.split('\\t')\n",
    "        query = tkns[0]\n",
    "        pos_answer_subgraph_head = int(tkns[1])\n",
    "        pos_answer_subgraph_tail = int(tkns[2])\n",
    "        results.append(Result(query, pos_answer_subgraph_head, pos_answer_subgraph_tail))\n",
    "# Create the training data\n",
    "data = np.zeros((len(results), 3), dtype=np.int)\n",
    "for i in range(len(results)):\n",
    "    query = results[i].query\n",
    "    tkns = query.split(' ')\n",
    "    r = int(tkns[1])\n",
    "    e = int(tkns[2])\n",
    "    data[i][0] = r\n",
    "    data[i][1] = e\n",
    "    pos = results[i].posh\n",
    "    if pos > 0:\n",
    "        if pos < 3:\n",
    "            data[i][2] = 1\n",
    "        elif  pos < 5:\n",
    "            data[i][2] = 2\n",
    "        elif pos < 10:\n",
    "            data[i][2] = 3\n",
    "        else:\n",
    "            data[i][2] = 4\n",
    "        #elif pos < 20:\n",
    "        #    data[i][2] = 4\n",
    "        #elif pos < 50:\n",
    "        #    data[i][2] = 5   \n",
    "        #elif pos < 100:\n",
    "        #    data[i][2] = 6\n",
    "# Take away 10% which should be used for the validation\n",
    "idx_val=np.random.choice(data.shape[0], int(data.shape[0]*0.10), replace=False)\n",
    "valid_data = data[idx_val,:]\n",
    "train_data = np.delete(data, idx_val, axis=0)\n",
    "\n",
    "classes = np.zeros(5)\n",
    "for t in train_data:\n",
    "    pos_h = t[2]\n",
    "    classes[pos_h] += 1\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a simple logistic regression model using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "n_input = dims * 2\n",
    "n_classes = 5\n",
    "n_hidden_1 = 256 # n neurons first layer\n",
    "n_hidden_2 = 256 # n neurons second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the input queues\n",
    "t_emb_e = tf.constant(emb_e)\n",
    "t_emb_r = tf.constant(emb_r)\n",
    "\n",
    "# Initialize the training data\n",
    "t = tf.constant(train_data)\n",
    "ds = tf.data.Dataset.from_tensor_slices(t)\n",
    "ds = ds.shuffle(buffer_size=100)\n",
    "ds = ds.batch(batch_size)\n",
    "\n",
    "# Initialize the valid data\n",
    "t_valid = tf.constant(valid_data)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices(t_valid)\n",
    "ds_valid = ds_valid.batch(batch_size)\n",
    "\n",
    "#iter = ds.make_initializable_iterator()\n",
    "iter = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "el = iter.get_next()\n",
    "\n",
    "# Lookup embeddings for the inputs\n",
    "rel, ent, y = tf.split(el, num_or_size_splits=3, axis=1)\n",
    "rel = tf.reshape(rel, shape=[-1]) # Flatten the tensor\n",
    "ent = tf.reshape(ent, shape=[-1]) # Flatten the tensor\n",
    "rel_emb = tf.nn.embedding_lookup(t_emb_r, rel)\n",
    "ent_emb = tf.nn.embedding_lookup(t_emb_e, ent)\n",
    "inp = tf.concat([rel_emb, ent_emb], axis=1)\n",
    "\n",
    "#Process the labels\n",
    "y = tf.reshape(y, shape=[-1])\n",
    "y_hot = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=tf.float64), dtype=tf.float64),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=tf.float64), dtype=tf.float64),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "}\n",
    "layer_1 = tf.add(tf.matmul(inp, weights['h1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "pred = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=y_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New operator to perform the predictions\n",
    "predictions = tf.nn.softmax(out_layer, name='predictions')\n",
    "# Compare the softmax with the actual values\n",
    "pred_indices = tf.argmax(predictions, axis=1)\n",
    "res = tf.equal(pred_indices, y)\n",
    "res = tf.cast(res, tf.int32)\n",
    "sres = tf.reduce_sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0  Loss=574.235329 Time=0.6429sec\n",
      "Correct predictions= 1164\n",
      "Train epoch 1  Loss=258.990702 Time=0.4262sec\n",
      "Train epoch 2  Loss=138.925642 Time=0.4271sec\n",
      "Train epoch 3  Loss=104.929911 Time=0.4478sec\n",
      "Train epoch 4  Loss=72.269823 Time=0.4202sec\n",
      "Train epoch 5  Loss=61.621947 Time=0.4148sec\n",
      "Train epoch 6  Loss=48.734735 Time=0.4108sec\n",
      "Train epoch 7  Loss=39.609300 Time=0.4053sec\n",
      "Train epoch 8  Loss=29.352749 Time=0.4053sec\n",
      "Train epoch 9  Loss=24.134601 Time=0.4119sec\n",
      "Train epoch 10  Loss=22.870800 Time=0.4035sec\n",
      "Correct predictions= 1341\n",
      "Train epoch 11  Loss=19.161370 Time=0.4067sec\n",
      "Train epoch 12  Loss=17.644384 Time=0.4063sec\n",
      "Train epoch 13  Loss=14.584012 Time=0.4058sec\n",
      "Train epoch 14  Loss=11.013591 Time=0.4113sec\n",
      "Train epoch 15  Loss=10.569966 Time=0.4032sec\n",
      "Train epoch 16  Loss=9.071127 Time=0.4027sec\n",
      "Train epoch 17  Loss=8.246739 Time=0.4031sec\n",
      "Train epoch 18  Loss=7.545294 Time=0.4212sec\n",
      "Train epoch 19  Loss=6.690223 Time=0.4279sec\n",
      "Train epoch 20  Loss=6.734526 Time=0.4382sec\n",
      "Correct predictions= 1400\n",
      "Train epoch 21  Loss=6.365276 Time=0.4122sec\n",
      "Train epoch 22  Loss=5.249115 Time=0.4054sec\n",
      "Train epoch 23  Loss=4.749368 Time=0.4121sec\n",
      "Train epoch 24  Loss=4.509664 Time=0.4044sec\n",
      "Train epoch 25  Loss=5.348511 Time=0.4064sec\n",
      "Train epoch 26  Loss=5.653716 Time=0.4159sec\n",
      "Train epoch 27  Loss=5.901140 Time=0.4049sec\n",
      "Train epoch 28  Loss=4.544937 Time=0.4114sec\n",
      "Train epoch 29  Loss=4.116850 Time=0.4124sec\n",
      "Train epoch 30  Loss=3.961913 Time=0.4090sec\n",
      "Correct predictions= 1409\n",
      "Train epoch 31  Loss=4.025757 Time=0.4064sec\n",
      "Train epoch 32  Loss=4.297714 Time=0.4056sec\n",
      "Train epoch 33  Loss=4.312463 Time=0.4095sec\n",
      "Train epoch 34  Loss=4.308303 Time=0.4234sec\n",
      "Train epoch 35  Loss=5.222499 Time=0.4194sec\n",
      "Train epoch 36  Loss=5.841602 Time=0.4084sec\n",
      "Train epoch 37  Loss=3.338064 Time=0.4080sec\n",
      "Train epoch 38  Loss=2.144001 Time=0.4166sec\n",
      "Train epoch 39  Loss=2.163612 Time=0.4288sec\n",
      "Train epoch 40  Loss=2.286700 Time=0.4297sec\n",
      "Correct predictions= 1359\n",
      "Train epoch 41  Loss=2.027527 Time=0.4063sec\n",
      "Train epoch 42  Loss=2.272371 Time=0.4039sec\n",
      "Train epoch 43  Loss=2.133529 Time=0.4083sec\n",
      "Train epoch 44  Loss=2.346254 Time=0.4095sec\n",
      "Train epoch 45  Loss=2.965862 Time=0.4087sec\n",
      "Train epoch 46  Loss=3.617287 Time=0.4071sec\n",
      "Train epoch 47  Loss=2.352977 Time=0.4031sec\n",
      "Train epoch 48  Loss=3.044222 Time=0.4038sec\n",
      "Train epoch 49  Loss=3.103338 Time=0.4002sec\n",
      "Train epoch 50  Loss=3.433111 Time=0.4076sec\n",
      "Correct predictions= 1215\n",
      "Train epoch 51  Loss=2.988291 Time=0.4136sec\n",
      "Train epoch 52  Loss=2.629272 Time=0.4039sec\n",
      "Train epoch 53  Loss=2.961561 Time=0.4099sec\n",
      "Train epoch 54  Loss=2.125096 Time=0.4056sec\n",
      "Train epoch 55  Loss=2.667157 Time=0.4133sec\n",
      "Train epoch 56  Loss=2.768045 Time=0.4052sec\n",
      "Train epoch 57  Loss=2.977920 Time=0.4088sec\n",
      "Train epoch 58  Loss=3.495680 Time=0.4053sec\n",
      "Train epoch 59  Loss=2.478559 Time=0.4024sec\n",
      "Train epoch 60  Loss=2.211736 Time=0.4162sec\n",
      "Correct predictions= 1419\n",
      "Train epoch 61  Loss=2.205523 Time=0.4164sec\n",
      "Train epoch 62  Loss=2.041482 Time=0.4311sec\n",
      "Train epoch 63  Loss=1.797916 Time=0.4172sec\n",
      "Train epoch 64  Loss=1.659511 Time=0.4176sec\n",
      "Train epoch 65  Loss=1.547703 Time=0.4120sec\n",
      "Train epoch 66  Loss=1.558606 Time=0.4228sec\n",
      "Train epoch 67  Loss=1.581880 Time=0.4145sec\n",
      "Train epoch 68  Loss=1.597956 Time=0.4107sec\n",
      "Train epoch 69  Loss=1.615920 Time=0.4130sec\n",
      "Train epoch 70  Loss=1.582200 Time=0.4118sec\n",
      "Correct predictions= 1484\n",
      "Train epoch 71  Loss=1.635346 Time=0.4127sec\n",
      "Train epoch 72  Loss=1.754939 Time=0.4113sec\n",
      "Train epoch 73  Loss=1.864809 Time=0.4130sec\n",
      "Train epoch 74  Loss=2.034209 Time=0.4145sec\n",
      "Train epoch 75  Loss=1.852385 Time=0.4151sec\n",
      "Train epoch 76  Loss=1.704274 Time=0.4306sec\n",
      "Train epoch 77  Loss=1.500143 Time=0.4354sec\n",
      "Train epoch 78  Loss=1.606872 Time=0.4271sec\n",
      "Train epoch 79  Loss=1.598711 Time=0.4200sec\n",
      "Train epoch 80  Loss=1.536237 Time=0.4207sec\n",
      "Correct predictions= 1499\n",
      "Train epoch 81  Loss=1.424206 Time=0.4230sec\n",
      "Train epoch 82  Loss=1.477682 Time=0.4152sec\n",
      "Train epoch 83  Loss=1.515358 Time=0.4198sec\n",
      "Train epoch 84  Loss=1.407647 Time=0.4192sec\n",
      "Train epoch 85  Loss=1.420311 Time=0.4164sec\n",
      "Train epoch 86  Loss=1.441922 Time=0.4281sec\n",
      "Train epoch 87  Loss=1.378365 Time=0.4189sec\n",
      "Train epoch 88  Loss=1.335403 Time=0.4227sec\n",
      "Train epoch 89  Loss=1.320519 Time=0.4162sec\n",
      "Train epoch 90  Loss=1.329050 Time=0.4185sec\n",
      "Correct predictions= 1491\n",
      "Train epoch 91  Loss=1.337690 Time=0.4442sec\n",
      "Train epoch 92  Loss=1.325683 Time=0.4261sec\n",
      "Train epoch 93  Loss=1.399068 Time=0.4288sec\n",
      "Train epoch 94  Loss=2.133482 Time=0.4354sec\n",
      "Train epoch 95  Loss=3.065776 Time=0.4371sec\n",
      "Train epoch 96  Loss=1.767643 Time=0.4242sec\n",
      "Train epoch 97  Loss=1.413340 Time=0.4193sec\n",
      "Train epoch 98  Loss=1.307527 Time=0.4317sec\n",
      "Train epoch 99  Loss=1.275074 Time=0.4247sec\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_input_initializer = iter.make_initializer(ds)\n",
    "valid_input_initializer = iter.make_initializer(ds_valid)\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    sess.run([init])\n",
    "    for epoch in range(training_epochs):\n",
    "        start = time.time()\n",
    "        sess.run(train_input_initializer)\n",
    "        # Loop over all batches\n",
    "        avg_loss = 0\n",
    "        num_batch = 0\n",
    "        while True:\n",
    "            try:\n",
    "                loss = sess.run([loss_op, train_op])                \n",
    "                num_batch += 1\n",
    "                avg_loss += loss[0]\n",
    "            except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                break\n",
    "            except e:\n",
    "                print(e)\n",
    "                break\n",
    "        print('Train epoch', epoch, \" Loss={:.6f}\".format(avg_loss / num_batch), \"Time={:.4f}sec\".format(time.time() - start))\n",
    "        if epoch % 10 == 0:\n",
    "            # Test the performance on the valid dataset\n",
    "            sess.run(valid_input_initializer)\n",
    "            num_batch = 0\n",
    "            correct = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    p = sess.run([sres])                \n",
    "                    correct += p[0]\n",
    "                    num_batch += 1\n",
    "                except (tf.errors.OutOfRangeError, StopIteration):\n",
    "                    break\n",
    "                except e:\n",
    "                    print(e)\n",
    "            print(\"Correct predictions=\", correct)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
